{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612fc418-cd31-4623-8f25-289f5cc5036d",
   "metadata": {},
   "source": [
    "# Final Project for INFO 6350\n",
    "### Project Team: Yun Zhou (yz2685)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20f341-5d43-4809-89e2-103e866d6ce0",
   "metadata": {},
   "source": [
    "# Part 0: Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1910d8-d118-4a03-9423-f7192d0081e3",
   "metadata": {},
   "source": [
    "**Statement of the problem:** The annual financial reports (10K reports) of the publicly traded companies are usually very lengthy and extensive, few people had the time and energy to read such report across all the companies over the years, and make accurate predications on the companies' financial performances based on the hundreds of reports every year of which the average page number can be up to 100 pages or 30k - 60k words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c98554-a3f1-4e7f-9681-2cb8df067897",
   "metadata": {},
   "source": [
    "**Impact:** The results from my analysis will deliver meaningful recommendations to not only the NLP scholars, but also to the real-world managers,  executives, and investors. By using text mining techniques, we can greatly improve the efficiency of the decision-making process in the business world, saving executives/managers' time reading their competitors' verbose 10K reports. Additionally, the project result will help the individual investors save time analyzing the 10K reports and make better investment decisions in the stock market.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff037b89-311c-4b4b-a4e8-a79c2ee6b30e",
   "metadata": {},
   "source": [
    "# Part 1: Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143c00e-bf8e-41d0-bf70-805ea339a712",
   "metadata": {},
   "source": [
    "### Q1: What are the tones or sentiments used to describe the risks of companies? Are they related to the stock prices? \n",
    "\n",
    "### Q2: What are the most important features in the **risk sections** in the 10-k reports that can make predictions or classifications on the companies' stock prices?\n",
    "\n",
    "### Q3: Given a risk section, can our classifier correctly determine if the record is before-2014 or post-2014?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c62638-6a16-4ac9-8d2b-4424f89dfb8c",
   "metadata": {},
   "source": [
    "# Part 2: Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc0723-f4bd-457c-9535-00ae8d72793d",
   "metadata": {},
   "source": [
    "**Methodology:** \n",
    "* Perform web crawling tasks to collect the financial reports over the years of the companies of interest. \n",
    "* Make gold labels based on the companies' financial metrics, such as earnings per share or net profit margin ratio. and label each report as below-average-margin vs. above-average-margin\n",
    "* Clean the corpus, remove tables, figures, stopwords, etc. \n",
    "* Perform sentiment analysis on the corpus to detect the positiveness and/or negativeness of the financial situations of the companies over the years. \n",
    "* Build regression models to predict the companies' financial performance and calculate R^2\n",
    "* Build various classifiers (e.g. random forest, decision tree, SVD, logistic classification, BERT) to classify the companies into below-average and above-average based on the financial metrics. Calculate the F1 and accuracy scores \n",
    "* Compare the results from regression models and classifers.\n",
    "* Make recommendations on how to choose the best stuitable model for such tasks in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314bb7d-eb9e-4887-9a69-bcbba0e76a52",
   "metadata": {},
   "source": [
    "# Part 3: Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55973c51-65a9-4f7c-a387-f5a05648497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import simplejson as json\n",
    "from urllib.request import Request, urlopen\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from   sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from   sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from   sklearn.model_selection import cross_val_score\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from   sklearn.cluster import KMeans, SpectralClustering, DBSCAN, OPTICS, AgglomerativeClustering\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from   collections import defaultdict\n",
    "from   nltk.corpus import stopwords\n",
    "from   sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d60a5-27d4-49e9-a6f7-f75b52d4bd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the base url needed to create the file url.\n",
    "base_url = r\"https://www.sec.gov\"\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36'\n",
    "}\n",
    "\n",
    "# four-digit year pattern\n",
    "yearPattern = re.compile(r'\\d{4}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc364f-fa4e-4a93-9f36-fa63d523f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cik = pd.read_csv('nasdaq100_ticker_cik_mapping.csv').fillna(value = 0)\n",
    "len(list_cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbe9bb-b4f7-478c-9246-4bff6381778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cik.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84839cf6-209e-441b-8147-7db1594507c9",
   "metadata": {},
   "source": [
    "## I: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297d2b2-3106-4750-b5cf-959a67cd96da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Scraping the SEC Query Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541d997-e211-4f1d-b192-3753ab39411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists to store the data scraped from the SEC website\n",
    "ciks = []\n",
    "risks = []\n",
    "years = []\n",
    "urls = []\n",
    "companies = []\n",
    "symbols = []\n",
    "\n",
    "# base URL for the SEC EDGAR browser\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf02652-0295-4e0e-8790-643b886bc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d513e4-ad48-44c4-9374-6e4c9dac2af4",
   "metadata": {},
   "source": [
    "### 1a. Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f9898-520c-4eee-a7e6-cf03b963d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the links to the 10k reports \n",
    "def get10kPages(url):\n",
    "    response = requests.get(url = url, headers=header)\n",
    "    soup10k = BeautifulSoup(response.content, 'html.parser')\n",
    "    # print(response)\n",
    "    # print(response.url)\n",
    "    return response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce853a-d2bf-4fce-8143-5c6de0b09456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get10kLinks(url, list_of_10ks):\n",
    "    response = requests.get(url = url, headers=header)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # for a in soup.find_all('a', href=True):\n",
    "        # url = a['href']\n",
    "        # print(url)\n",
    "        \n",
    "    suffix = \"htm\";\n",
    "\n",
    "    tables = soup.find('table')\n",
    "    rows = tables.find_all('tr')\n",
    "    if len(rows) > 0:\n",
    "        row10k = rows[1] # row 1 has link to 10k report\n",
    "        # print(row10k)\n",
    "        for a in row10k.find_all('a', href=True):\n",
    "            url = a['href']\n",
    "            if url.endswith(suffix):\n",
    "                list_of_10ks.append(\"https://www.sec.gov\"+ url)\n",
    "                # print(\"https://www.sec.gov\"+ url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178535a-5611-4a51-8a1d-0ab12716395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f0b6c-a273-405e-ab46-e8ca13a8d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will scrap the 10k report \n",
    "def scrap10k(url, cik, company, symbol):\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(2) # give browser some time to load the js \n",
    "\n",
    "    html = driver.page_source\n",
    "    sp = BeautifulSoup(html)\n",
    "    \n",
    "    text = \"\"\n",
    "    for d in sp.find_all(text=True):\n",
    "        text += d.get_text()\n",
    "    \n",
    "    # cleaning \n",
    "    # print(text)\n",
    "    text = text.replace(u'\\xa0', u' ').lower()\n",
    "                \n",
    "    # extract risk factor sections only\n",
    "    # print(url)\n",
    "    \n",
    "    # get fiscal year     \n",
    "    yr = \"\"\n",
    "    for span in sp.find_all(text=True):\n",
    "        stext = span.text\n",
    "        if (stext.find(\"January\") != -1\n",
    "         or stext.find(\"February\") != -1\n",
    "         or stext.find(\"March\") != -1\n",
    "         or stext.find(\"April\") != -1\n",
    "         or stext.find(\"May\") != -1\n",
    "         or stext.find(\"June\") != -1\n",
    "         or stext.find(\"July\") != -1\n",
    "         or stext.find(\"August\") != -1\n",
    "         or stext.find(\"September\") != -1\n",
    "         or stext.find(\"October\") != -1\n",
    "         or stext.find(\"November\") != -1\n",
    "         or stext.find(\"December\") != -1):\n",
    "            stext = stext.strip()\n",
    "            yr = stext[-4:]\n",
    "            # print(yr)\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # print(\"=================\")    \n",
    "    # print(text)\n",
    "    \n",
    "    # proceed only when a valid year is scraped \n",
    "    yr_match = re.match(yearPattern, yr)\n",
    "    if yr_match != None:\n",
    "        yr = int(yr) # convert string to int\n",
    "        if yr > 2006:\n",
    "            start = find_nth(text, \"item 1a.\", 2)\n",
    "            # print(\"start index\", start)\n",
    "            end = find_nth(text, \"item 1b.\", 2)\n",
    "            # print(\"end index\", end)\n",
    "            substring = text[start:end]\n",
    "            if len(substring) > 100: # only pull longer risk factors \n",
    "                risks.append(substring)\n",
    "                years.append(yr)\n",
    "                ciks.append(cik)\n",
    "                urls.append(url)\n",
    "                companies.append(company)\n",
    "                symbols.append(symbol)\n",
    "    #     else:\n",
    "    #         print(\"year before 2007: \", yr)\n",
    "    # else:\n",
    "    #     print(\"fiscal year not detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a4587-f17a-42e9-8ef1-561e959acd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataByCIK(cik, company, symbol):\n",
    "    ########################################\n",
    "    ### Step 1. Scraping the SEC Query Page\n",
    "    ########################################\n",
    "    # define our parameters dictionary\n",
    "    param_dict = {'action':'getcompany',\n",
    "                  'CIK': cik,\n",
    "                  'type':'10-k',\n",
    "                  'dateb':'20230101',\n",
    "                  'owner':'exclude',\n",
    "                  'start':'',\n",
    "                  'output':'',\n",
    "                  'count':'100'}\n",
    "\n",
    "    # request the url, and then parse the response.\n",
    "    response = requests.get(url = endpoint, params = param_dict, headers=header)\n",
    "    # response = requests.get(url = endpoint, params = param_dict)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # print('Request Successful')\n",
    "    # print(response.url)\n",
    "    \n",
    "    doc_table = soup.find_all(class_ = \"blueRow\")\n",
    "    \n",
    "    data = soup.find_all(class_='blueRow')\n",
    "\n",
    "    list_10k = []\n",
    "\n",
    "    for i, row in enumerate(data): \n",
    "        for a in data[i].find_all('a', href=True):\n",
    "            url = a['href']\n",
    "            if (url.startswith('/Archives/edgar/')):\n",
    "                list_10k.append(\"https://www.sec.gov\"+ url)\n",
    "                # print(\"https://www.sec.gov\"+ url)\n",
    "                \n",
    "    ########################################\n",
    "    ### Step 2. Scraping Company Page \n",
    "    ########################################\n",
    "    \n",
    "    list_of_links = []\n",
    "    for link in list_10k:\n",
    "        list_of_links.append(get10kPages(link))\n",
    "        \n",
    "    # get the url for 10k report \n",
    "    list_of_10ks = []\n",
    "    for url in list_of_links:\n",
    "        get10kLinks(url, list_of_10ks)\n",
    "        \n",
    "\n",
    "    ########################################\n",
    "    ### Step 3. Scraping 10k Reports \n",
    "    ########################################\n",
    "    for report in list_of_10ks:\n",
    "        scrap10k(report, cik, company, symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18cefe-90ab-4169-adba-0b326ae380fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# scrap10k('https://www.sec.gov/Archives/edgar/data/1018724/000101872419000004/amzn-20181231x10k.htm', \"1018724\", \"amzn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133580b4-c024-4272-8ee6-89212be52158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# getDataByCIK(\"1067983\", \"brk-b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28338dab-4493-4e84-b69a-e7a699b0b58d",
   "metadata": {},
   "source": [
    "### 1b. Run scripts for all the companies of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730751e-d1ad-40a1-8408-e69f64ea0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, row in list_cik.iterrows():\n",
    "    getDataByCIK(row.cik, row.company, row.symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e58dc-f2ac-4712-9dbf-7efb813c0ef1",
   "metadata": {},
   "source": [
    "### 1c. Create Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777126e-d3c3-4586-9c12-c97d10a778b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame\n",
    "data_tuples = list(zip(ciks, symbols, companies, years, risks, urls))\n",
    "# data_tuples\n",
    "\n",
    "df = pd.DataFrame(data_tuples, columns=['cik', 'symbol', 'company', 'fiscal_year', 'risk', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3897e23-fa28-4953-b5fe-de54cf3a7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d330b0c-1d36-4f05-b6b7-f89f213ffd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93474b21-a6e5-448a-b0f9-f219f95a94b4",
   "metadata": {},
   "source": [
    "## II. Scrap stock prices from Yahoo Finance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c762e5-0d53-461a-ad14-0bf29a892538",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fi = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce6566-2c77-429e-bb9b-dee0107823e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStockPricesByTicker(ticker, fiscal_year, period1, period2):\n",
    "    url_history = f'https://finance.yahoo.com/quote/{ticker}/history?period1={period1}&period2={period2}&interval=1mo&filter=history&frequency=1mo&includeAdjustedClose=false'\n",
    "    \n",
    "    respf = requests.get(url_history, headers=header)\n",
    "    # print(respf)\n",
    "    \n",
    "    soupf = BeautifulSoup(respf.text, 'html.parser')\n",
    "    \n",
    "    patternf = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = soupf.find('script', text=patternf).contents[0]\n",
    "    \n",
    "    start = script_data.find(\"context\")-2\n",
    "    json_data = json.loads(script_data[start:-12])\n",
    "    \n",
    "    # get historical stock prices \n",
    "    try:\n",
    "        HistoricalPriceStore = json_data['context']['dispatcher']['stores']['HistoricalPriceStore']\n",
    "        # print(HistoricalPriceStore)\n",
    "        # clean the stock price data \n",
    "        close = -99999 #default\n",
    "        for row in HistoricalPriceStore['prices']:\n",
    "            date = row['date']\n",
    "            dt_formatted = datetime.datetime.fromtimestamp(date).date() # using the local timezone\n",
    "\n",
    "            # get the stock price in December of a given year \n",
    "            if dt_formatted.month == 12 and dt_formatted.year == fiscal_year and 'close' in row:\n",
    "                # debug\n",
    "                # print(ticker, \"=========\", fiscal_year, \"=====\", row['date'])\n",
    "                # print(HistoricalPriceStore)\n",
    "                close = row['close']\n",
    "                break # once find a December record then stop looking\n",
    "\n",
    "        if dict_fi.get(ticker) == None:\n",
    "            dict_fi[ticker] = {}\n",
    "\n",
    "        dict_fi[ticker][fiscal_year] = close\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd154df8-3b62-4bde-9e97-b784311ca283",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2b. Run scripts for all the records collected from #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa05ccc-1eed-404e-aa6a-e749ab35670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # test\n",
    "# for i, row in df.iterrows():\n",
    "#     if row.symbol == 'AVGO':\n",
    "#         ticker = row.symbol\n",
    "#         fiscal_year = row.fiscal_year\n",
    "#         # period1 = '1167609600' # 2007-01-01\n",
    "#         # period2 = '1672444800' # 2022-12-30\n",
    "#         getStockPricesByTicker('AVGO', fiscal_year, '1167609600', '1672444800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683596e-84c8-44a7-a8ba-33e1e2ab5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['symbol'] == 'AVGO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf39f2-54e9-4fc6-9f0c-a0665b9ed08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, row in df.iterrows():\n",
    "    ticker = row.symbol\n",
    "    fiscal_year = row.fiscal_year\n",
    "    # period1 = '1167609600' # 2007-01-01\n",
    "    # period2 = '1672444800' # 2022-12-30\n",
    "    getStockPricesByTicker(ticker, fiscal_year, '1167609600', '1672444800')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacd74c-c1cc-4fba-a23e-eddf779887d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2c. Create data frames for stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c9e7d-03c2-4c6b-b7e8-834a3c0139d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d1878-6625-4230-ba6d-7b970af88841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi = pd.DataFrame.from_records(\n",
    "    [\n",
    "        (level1, level2, leaf)\n",
    "        for level1, level2_dict in dict_fi.items()\n",
    "        for level2, leaf in level2_dict.items()\n",
    "    ],\n",
    "    columns=['symbol', 'fiscal_year', 'price']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b9346-9719-45f2-9a17-a2077854df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f7b32-15ed-4590-a7a6-86547553ede8",
   "metadata": {},
   "source": [
    "### 2d. Merge two data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6b224-8e0f-43c3-b207-70dfc76ba29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, df_fi, how='left', on=['fiscal_year','symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d01709-1507-455c-a885-bb8363e402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1cf50-a0b4-421e-bde5-d75e6f268a53",
   "metadata": {},
   "source": [
    "## III: Exploratory Analysis & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2de677-e64e-4344-afa7-4aa547863e92",
   "metadata": {},
   "source": [
    "### 0. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec82d70-dbad-4f2e-ace7-afc3e97efca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    counter = Counter()\n",
    "    tokens_nltk = word_tokenize(text)\n",
    "\n",
    "    for token in tokens_nltk:\n",
    "        counter[token] += 1\n",
    "    return sum(counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd7aa5-1db8-43f8-a4a2-7ad33f100159",
   "metadata": {},
   "source": [
    "### 1. Exclude invalid stock prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89944503-2a66-419b-88ea-e649002132ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['price'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc58ff-307f-4e21-ba57-19587d3b326d",
   "metadata": {},
   "source": [
    "### 2. Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb58a97-ea43-4bad-b5b0-5b754a4b74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "text_len = []\n",
    "for text in merged_df['risk']:\n",
    "    text = unicodedata.normalize('NFKD', text.replace(\"\\'\", \"'\").replace(\"\\ in\\ form\", \" inform\").replace(\"\\n\", \" \").lower().strip())\n",
    "    text_list.append(text)\n",
    "    cnt = word_count(text)\n",
    "    text_len.append(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164bae3-e1c7-4329-afdb-ace28c1c1610",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Add more metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af6b94-2dcd-4ad0-a547-0837cc368d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cleaned text as a column to the df\n",
    "merged_df['text'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944a577-e619-4794-b649-8c97588b74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['word_count'] = text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce14b3f-23a0-45a4-8361-e9814349e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b0c5c-5970-4372-8771-f831fe0e5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953e793-9acf-45a9-aa3a-0ab26aa69d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "plt.hist(merged_df.fiscal_year, bins=merged_df.fiscal_year.nunique())\n",
    "plt.title(\"Histogram of fiscal year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7eb2e-dbdb-4a8b-b92a-68964947eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,8))\n",
    "plt.hist(merged_df.price, bins=merged_df.price.nunique())\n",
    "plt.title(\"Histogram of stock price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37c962-d70a-4c95-9068-3332d5ee9d0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### It seemed most of the stock prices fall into <= USD 250 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a338b-19b5-40f4-bda5-5925882e7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depicting the visualization\n",
    "plt.scatter(merged_df.word_count, merged_df.price) \n",
    "plt.xlabel('word count') \n",
    "plt.ylabel('stock price') \n",
    "plt.title(\"Linear graph word count vs. stock price\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac8652-e8bc-4824-921c-498896fb6fc6",
   "metadata": {},
   "source": [
    "#### From the graph above, we couldn't find any obvious correlation between word count and stock prices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c085316-6ac5-41a3-9296-d113ab0957d4",
   "metadata": {},
   "source": [
    "#### I pulled the companies with stock prices above USD 750 for sanity check. It seemed they were accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72891c0-cb06-4842-b37d-f668b37b5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df.price > 750]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749ff0a-c146-41a1-8215-339ec37d3afe",
   "metadata": {},
   "source": [
    "#### Next, I removed the duplicates in the data frame based on cik and fiscal_year, and removed outliers with stock prices greater than USD 250. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83ba6b-e879-4af1-9bef-a0427b6a3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merged_df.drop_duplicates(subset=['cik', 'fiscal_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377f744-d30b-4571-af8f-593558631131",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.price <= 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e4700-626c-4197-a660-b2ac163214fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e4365-7de8-49f9-8c12-dd73cbc8de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depicting the visualization after dropping duplicates and outliners \n",
    "plt.scatter(data.word_count, data.price) \n",
    "plt.xlabel('word count') \n",
    "plt.ylabel('stock price') \n",
    "plt.title(\"Linear graph word count vs. stock price\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ae3b4-0f7e-4772-a4da-53ea1a0d12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depicting the visualization\n",
    "plt.scatter(data.fiscal_year, data.word_count, color=\"red\") \n",
    "plt.xlabel('fiscal year') \n",
    "plt.ylabel('word count') \n",
    "plt.title(\"Linear graph word count vs. fiscal year\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ff2b-74c8-4bd8-9e96-3976c8692ce9",
   "metadata": {},
   "source": [
    "#### From the graph above, we couldn't find obvious trend of either decreasing or increasing word count as time goes by. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8bf7b-c9c6-4606-ae3f-9e7ba763b205",
   "metadata": {},
   "source": [
    "### 4. Generate a word cloud based on all the risk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba404c-9f43-4d17-beb6-caef3f0a1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to string and generate\n",
    "all_risks=(\" \").join(data.risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18496b23-b58c-4e80-a828-054e9336ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 1000, height = 500, background_color=\"white\").generate(all_risks)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"wordcloud\"+\".png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf27c0-892e-4c82-b757-1ff2d47f2eb3",
   "metadata": {},
   "source": [
    "### 5. Save the cleaned data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d99e9-1beb-4e56-a638-ec9e8c99110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data.csv\", encoding='utf-8', sep=',', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de063831-3700-443c-a8e0-30b09b3c5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7e012-c449-46e5-985c-f247756ef6e7",
   "metadata": {},
   "source": [
    "## IV: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63f21c-3552-4730-8d48-33bfd87641e9",
   "metadata": {},
   "source": [
    "### 0-0. Create gold labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d601e-d9d6-40ae-94db-35ddd426d8a1",
   "metadata": {},
   "source": [
    "* Create a vector **y_binary** of gold labels for stock prices. 0 stands for below-average stock prices and 1 represents above-average stock prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25537d91-2d35-49c9-a7f8-942d2745372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = list(map(lambda x : 0 if x < np.mean(data.price) else 1, data['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca40d63-5ffd-4cd5-9da3-4e799f71ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e584fc3-01d8-4342-be4d-d34dc9cbe0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_binary) / len(y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c9a85-782e-4dab-a5e0-f6523fd165ef",
   "metadata": {},
   "source": [
    "### 0-1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62851541-3512-45e9-9d30-04ebb40d08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## code from INFO 6350 problem set code ##############\n",
    "def plot_compare(X, labels, title, reduce=True, alpha=0.2):\n",
    "    '''\n",
    "    Takes an array of object data, a set of cluster labels, and a title string\n",
    "    Reduces dimensions to 2 and plots the clustering.\n",
    "    Returns nothing.\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from   sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "    if reduce:\n",
    "        # TruncatedSVD is fast and can handle sparse inputs\n",
    "        # PCA requires dense inputs; MDS is slow\n",
    "        coordinates = TruncatedSVD(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        # Optionally handle 2-D inputs\n",
    "        coordinates = X\n",
    "    \n",
    "    # Set up figure\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    # Unlabeled data\n",
    "    plt.subplot(121) # 1x2 plot, position 1\n",
    "    plt.scatter(\n",
    "        coordinates[:, 0], \n",
    "        coordinates[:, 1], \n",
    "        alpha=alpha, # Set transparency so that we can see overlapping points\n",
    "        linewidths=0 # Get rid of marker outlines\n",
    "    )\n",
    "    plt.title(\"Unclustered data\")\n",
    "\n",
    "    # Labeled data\n",
    "    plt.subplot(122)\n",
    "    sns.scatterplot(\n",
    "        x=coordinates[:, 0], \n",
    "        y=coordinates[:, 1],\n",
    "        hue=labels,\n",
    "        alpha=alpha,\n",
    "        palette='viridis',\n",
    "        linewidth=0\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14680e2-adc5-43af-93a7-d8bf375be1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## code from INFO 6350 problem set code ##############\n",
    "def pull_samples(texts, labels, n=3):\n",
    "    '''\n",
    "    Takes lists of texts and an array of labels, as well as number of samples to return per label.\n",
    "    Prints sample texts belonging to each label.\n",
    "    '''\n",
    "    texts_array = np.array(texts) # Make the input text list easily addressable by NumPy\n",
    "    for label in np.unique(labels): # Iterate over labels\n",
    "        print(\"Label:\", label)\n",
    "        sample_index = np.where(labels == label)[0] # Limit selection to current label\n",
    "        print(\"Number of texts in this cluster:\", len(sample_index), '\\n')\n",
    "        chosen = np.random.choice(sample_index, size=n) # Sample n texts with this label\n",
    "        for choice in chosen:\n",
    "            print(\"Sample text:\", choice)\n",
    "            # print(str(texts_array[choice]).split(\" 0\")[0], '\\n') # Print each sampled text\n",
    "            print(str(texts_array[choice])[1:80], '\\n') # Print each sampled text\n",
    "        print(\"###################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391ed05-b674-4a88-a806-989e6ffb93b5",
   "metadata": {},
   "source": [
    "### Q1: What are the tones or sentiments used to describe the risks of companies? Are they related to the stock prices? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca0a91-07e8-483d-8bea-4aa6fe0599db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1-1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1edc0-96a4-407b-8ef5-0a358826ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making stopwords list\n",
    "stoplist = stopwords.words('english')\n",
    "for el in [i for i in string.punctuation]:\n",
    "    stoplist.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d78d9-efb1-4555-ae58-65b771a221db",
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_file = os.path.join('emolex.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790eb0c-e530-462c-be70-f20f579a1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## code from INFO 6350 problem set code ##############\n",
    "def read_emolex(filepath=None):\n",
    "    '''\n",
    "    Takes a file path to the emolex lexicon file.\n",
    "    Returns a dictionary of emolex sentiment values.\n",
    "    '''\n",
    "    if filepath==None: # Try to find the emolex file\n",
    "        filepath = os.path.join('..','..','data','lexicons','emolex.txt')\n",
    "        if os.path.isfile(filepath):\n",
    "            pass\n",
    "        elif os.path.isfile('emolex.txt'):\n",
    "            filepath = 'emolex.txt'\n",
    "        else:\n",
    "            raise FileNotFoundError('No EmoLex file found')\n",
    "    emolex = defaultdict(None) # Like Counter(), defaultdict eases dictionary creation\n",
    "    with open(filepath, 'r') as f:\n",
    "    # emolex file format is: word emotion value\n",
    "        for line in f:\n",
    "            word, emotion, value = line.strip().split()\n",
    "            if emolex.get(word) == None:\n",
    "                emolex[word] = {}\n",
    "            emolex[word][emotion] = int(value)\n",
    "    return list(emolex.items())[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334545a5-c0fd-49a6-a8a7-435a087c8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EmoLex data. Make sure you set the right file path above.\n",
    "emolex = read_emolex(emolex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd7cfa-30ae-49e4-9305-31cbf6c5963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_sentiment_score from INFO 6350 problem set code\n",
    "def sentence_sentiment_score(toks, lexicon = emolex):\n",
    "    total = 0\n",
    "    emo_dict = defaultdict(lambda: 0)\n",
    "    \n",
    "    emotions = ['anger', 'anticipation','disgust','fear','joy','negative','positive','sadness','surprise', 'trust']\n",
    "    \n",
    "    \n",
    "    for word in toks:\n",
    "            total += 1\n",
    "            for emotion in emotions:\n",
    "                try:\n",
    "                    emo_dict[emotion] += lexicon[word][emotion]\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        if total > 0:\n",
    "            emo_dict[emotion] /= total\n",
    "        \n",
    "    return emo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79783ec-c196-4cd5-8434-e949e637fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentScore(sentence_dicts, data, index):\n",
    "    \n",
    "    sum_sent_dict =  {'anger': 0 , 'anticipation': 0,'disgust': 0,'fear': 0,'joy': 0,'negative': 0,'positive': 0,'sadness': 0,'surprise': 0, 'trust': 0}\n",
    "    \n",
    "    for sentence_dict in sentence_dicts:\n",
    "        for emotion in sentence_dict:\n",
    "            sum_sent_dict[emotion] += sentence_dict[emotion]\n",
    "    \n",
    "    for emotion in sum_sent_dict.keys():\n",
    "        sum_sent_dict[emotion] /= len(sentence_dicts)\n",
    "        data.at[index, emotion] = sum_sent_dict[emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf0b1e-6137-4309-b55d-9f7b5b9868c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_text from INFO 6350 problem set code\n",
    "def tokenize_text(text, stops=[]):\n",
    "    sentences = []\n",
    "    for sent in sent_tokenize(text.lower()):\n",
    "        sentences.append([word for word in word_tokenize(sent) if word not in stops])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3c2b6-0249-473f-aa71-02c251809f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90432cc-8320-475a-9288-95f4d56c7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adding sentiment score columns\n",
    "size = len(data)\n",
    "\n",
    "data['anger'] = np.zeros(size)\n",
    "data['anticipation'] = np.zeros(size)\n",
    "data['disgust'] = np.zeros(size)\n",
    "data['fear'] = np.zeros(size)\n",
    "data['joy'] = np.zeros(size)\n",
    "data['negative'] = np.zeros(size)\n",
    "data['positive'] = np.zeros(size)\n",
    "data['sadness'] = np.zeros(size)\n",
    "data['surprise'] = np.zeros(size)\n",
    "data['trust'] = np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd01281-accc-4c88-ae2b-7f458259d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for index, text in enumerate(data['risk']):\n",
    "    sentence_dicts = []\n",
    "    for sentence in tokenize_text(text, stops=stoplist):\n",
    "        sentence_dicts.append(sentence_sentiment_score(sentence))\n",
    "    getSentScore(sentence_dicts, data, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7de71-8a72-40c0-a089-44acc226bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2669e-3f04-416d-a575-b375d77b5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(\n",
    "    encoding = 'utf-8',\n",
    "    strip_accents = 'unicode',\n",
    "    lowercase = True,\n",
    "    min_df = 0.01,\n",
    "    max_df = 0.9,\n",
    "    use_idf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f674a-b433-4d60-9087-0a0c8ed17ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.fiscal_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3f2bc-9fa9-4c92-8dd7-8d6e71ed4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform vectorization\n",
    "X = vectorizer.fit_transform(data.risk.values.astype('U'))\n",
    "print(\"Shape of the feature matrix\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ada3c8-bc8b-4982-940f-f4a5fe06355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard-scale feature matrix\n",
    "X = StandardScaler().fit_transform(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27415e14-e915-44b9-9767-1d517ef32457",
   "metadata": {},
   "source": [
    "### 1-2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c115671-4163-47f0-b613-cd17632f4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = KMeans(n_clusters=2).fit_predict(X) # this output is the cluster labels\n",
    "\n",
    "# Print label vector shape\n",
    "print('Label vector shape: ', y_kmeans.shape)\n",
    "\n",
    "print(\"Using KMeans clustering with n=2 clusters; we are assuming that the clusters are detective and non-detective novels.\")\n",
    "\n",
    "# Plot results\n",
    "plot_compare(X, y_kmeans, 'k-Means (predicted) labels', reduce=True, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f33bf5-ff38-488f-acbb-b29ea3701da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_samples(data, y_kmeans, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72537cd7-5245-4fc5-9448-5f99901aab55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9544ed48-0fbf-4ff8-8707-90af9526e405",
   "metadata": {},
   "source": [
    "### Q2: What are the most important features in the **risk sections** in the 10-k reports that can make predictions or classifications on the companies' stock prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224be1e3-88f8-45cc-b512-f74a5bd96bc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-1. Build a token-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c432e3e-dcb5-455a-ad5d-223bdb4b8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select best features\n",
    "selector = SelectKBest(score_func=mutual_info_regression, k=50)\n",
    "\n",
    "# Print the shape of your new feature matrix\n",
    "X_top = selector.fit_transform(X, y_binary)\n",
    "print(\"Shape of the combined matrix with 300 selected features: \", X_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f51019-c085-4086-93aa-574d089a232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a 10-fold cross-validated accuracy score using a logistic regression classifier on your selected feature data.\n",
    "# Cross-validate the logistic regression classifier on full input data\n",
    "print(\"Mean cross-validated accuracy scores:\", \n",
    "      np.mean(cross_val_score(LogisticRegression(), X_top, y_binary, scoring='accuracy', cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d4cab-3a83-4aa8-b75e-6104404d6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_names_ = [feature_names[i] for i in selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfc5b8-b921-4665-8a82-2431e33f24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1657c6c-be23-4e15-8fcd-0014c199cab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-2. Build a word-embedding-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3ba6d-1dc6-4fab-8df6-d63c9cc701b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(doc, nlp):    \n",
    "    # remove_noninformative_tokens\n",
    "    tokens = nlp(doc)\n",
    "    culled = []\n",
    "    culled = [token for token in tokens if not (token.is_stop or token.is_punct or token.is_space) and token.has_vector]\n",
    "    '''\n",
    "    Takes two lists of spacy token objects.\n",
    "    Returns cosine similarity between their embedding representations.\n",
    "    '''\n",
    "    mean_vector_culled = np.mean([token.vector for token in culled], axis=0)\n",
    "       \n",
    "    return mean_vector_culled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61502c2b-4ab2-4649-9e3f-4ee03034a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\") # Note '_lg' = large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f694bd9-41d2-4578-9927-cef23622700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_embedding = []\n",
    "X_embedding = np.zeros((len(data.risk), nlp.vocab.vectors_length))\n",
    "\n",
    "for i, content in enumerate(data.risk):\n",
    "    X_embedding[i] = get_doc_embedding(content, nlp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfe195-b7db-48d2-8781-033dcd829e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of embedding matrix: \", X_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3c4dc-7acc-4b1d-996b-dbf4416ec690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard-scale feature matrix\n",
    "X_embedding = StandardScaler().fit_transform(X_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6d3b9-c456-4ce5-8dfe-ff384f514488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate a 10-fold cross-validated accuracy score using a logistic regression classifier on your selected feature data.\n",
    "# Cross-validate the logistic regression classifier on full input data\n",
    "\n",
    "print(\"Mean cross-validated accuracy scores:\", \n",
    "      np.mean(cross_val_score(LogisticRegression(max_iter=500), X_embedding, y_binary, scoring='accuracy', cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925c88d-e985-4d74-95b5-aff9eaf7b5ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-3. Evaluate regression performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858f1ba-d985-48c1-8074-d3aa92fd72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token-based Mean 10-fold cross-validated R^2:\", \n",
    "      np.mean(cross_val_score(LinearRegression(), X_top300, y_binary, scoring='r2', cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe360aeb-81bc-4b62-b7e4-c887899348ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding-based Mean 10-fold cross-validated R^2:\", \n",
    "      np.mean(cross_val_score(LinearRegression(), X_embedding, y_binary, scoring='r2', cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce4580-07ef-453d-b0cf-fb8e9bc135cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-4. Improve classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132a4b0-c968-4ec2-aff7-733295452850",
   "metadata": {},
   "source": [
    "#### 2-4-1. Improve token-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a35ad5-d0d4-447c-8242-28d22c0dbd0d",
   "metadata": {},
   "source": [
    "##### Feature Engineering\n",
    "* Let's increase the number of features from 300 to 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19300068-537e-43f8-8a27-451af2dd62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_k = SelectKBest(score_func=mutual_info_regression, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abccb20-4a98-4733-989e-a1f773f22e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of your new feature matrix\n",
    "X_token_k = selector_k.fit_transform(X, y_binary)\n",
    "print(\"Shape of the matrix with 20 selected features: \", X_token_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26314ad9-0ac7-4238-9cec-fef92c7fec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean cross-validated accuracy scores:\", \n",
    "      np.mean(cross_val_score(LogisticRegression(), X_token_k, y_binary, scoring='accuracy', cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8186d-fd5e-4f46-92d7-84e304a48979",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Random Forest === Mean cross-validated accuracy scores:\", \n",
    "      np.mean(cross_val_score(RandomForestClassifier(max_features=\"auto\"), X_token_k, y_binary, scoring='accuracy', cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329f9b0-ab4b-4657-8438-1b28101431fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Decision Tree === Mean cross-validated accuracy scores:\", \n",
    "      np.mean(cross_val_score(DecisionTreeClassifier(max_depth=100), X_token_k, y_binary, scoring='accuracy', cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366781d-29db-46ac-8f38-c6814596714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_k = [feature_names[i] for i in selector_k.get_support(indices=True)]\n",
    "feature_names_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c4290-2343-4027-92c6-44a47c4d4399",
   "metadata": {},
   "source": [
    "#### 2-4-2. Improve embedding-token-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6417e12-8df6-45db-ab66-9d1e1f8eb8b8",
   "metadata": {},
   "source": [
    "##### Try SVM classifier\n",
    "* linear SVM \n",
    "* non-linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5872e54-e96d-4709-8b26-1e4ccfc29cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Mean cross-validated accuracy scores:\", \n",
    "      np.mean(cross_val_score(SVC(), X_embedding, y_binary, scoring='accuracy', cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4062227-eb72-490a-b0c7-25740fa4dd95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-5. Conclusion on Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eee20d-1bc0-4e39-85c7-8c2df0df10ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02ed25-ac5f-45d1-93f7-4cd1d65a6db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b674695-29c2-4f76-a6a8-15681e0b068d",
   "metadata": {},
   "source": [
    "# Part 4: Results and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b5310-57c9-4c11-b2d1-7541ab4dd8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4696662-2b81-4589-bcf8-eb8750ffd8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795da3a2-b307-4151-8101-33ebdc1e5c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d4fff-7a69-4ad1-bca3-7d30dc067340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cde9a-b9b4-47f5-a526-7fef6140abab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f4e1d30-554b-48ee-a100-12ebec127afa",
   "metadata": {},
   "source": [
    "# Part 5: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847623f-5e0d-453f-a6a6-3f79f340fbd5",
   "metadata": {},
   "source": [
    "# Part 6: References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dd98c-4390-46fb-ab4c-0958e505998b",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/48687857/python-json-list-to-pandas-dataframe\n",
    "\n",
    "* https://www.youtube.com/watch?v=fw4gK-leExw&ab_channel=IzzyAnalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02231ca4-0f3e-4db5-88a7-7045e06d2469",
   "metadata": {},
   "source": [
    "# Part 7: Responsibility Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb5965-c844-409e-88f1-05c6f7ffc7a1",
   "metadata": {},
   "source": [
    "I completed this project on my own. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
